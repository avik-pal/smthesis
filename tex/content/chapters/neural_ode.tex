\chapter{Neural Ordinary Differential Equations}
\label{chapter:neural_ode}

\section{Ordinary Differential Equations}
\label{sec:ordinary_differential_equations}

Ordinary Differential Equations (ODEs) are equations defined by a relationship to their derivative. We can generally write an ODE as:
%
\begin{equation}
  \frac{dz}{dt} = \func{f}{z, p, t}
\end{equation}
%
This effectively describes the evolution of a state $\func{z}{t}$. To compute $\func{z}{t}$, we could solve the following integral equation:
%
\begin{equation}
  \func{z}{t} = \int_{t_0}^{t} \func{f}{z, p, t} \, \dt
\end{equation}
%
where $t_0$ is the initial time. However, computing these solutions analytically is almost impossible, and hence we need to rely on numerical solvers. For this thesis, we will focus exclusively on Initial Value Problems (IVP) which specify the differential equations along with an initial condition, i.e., the value of the state at $t_0$: $z(t_0)$. There are other kinds of ODE Problems like Boundary Value Problems (BVP) which specify additional conditions at the end of the interval, i.e., $z(t_1)$.

Numerical Solvers for ODEs can be broadly categorized into Implicit Methods and Explicit Methods. Explicit Methods compute the state of the dynamical system at a future time-point given the current state. Implicit Methods solve for the state of the dynamical system at a future time-point given the current state \textit{and the later one}. For example, consider two extremely simple numerical solvers for an ODE:
%
\begin{itemize}
  \item Euler Method (Explicit Method)~\citep{euler1824institutionum}:
        %
        \begin{equation}
          z_{n + 1} = z_n + \dt \cdot \func{f}{z_n, p, t_n}
        \end{equation}
        %
  \item Backward Euler Method (Implicit Method)~\citep{euler1824institutionum}:
        %
        \begin{equation}
          z_{n + 1} = z_n + \dt \cdot \func{f}{z_{n + 1}, p, t_{n + 1}}
        \end{equation}
        %
\end{itemize}
%
Explicit methods tend to be faster than implicit methods but are not effective for solving stiff equations~\citep{wanner1996solving,kim2021stiff}. In this thesis, we will exclusively use explicit methods. A detailed discussion on implicit and explicit methods is beyond the scope of this thesis, and we refer the readers to \citet{rackauckas2019scimlbook}.

\section{Explicit ODE Solvers}
\label{subsec:explicit_ode_solvers}

In this section, we will briefly discuss ODE solvers that are relevant in the context of Neural ODEs. %  We start with Euler Method~(\Cref{subsec:euler_method}) to give a simple introduction to numerical solvers. Then we build upon that to describe the Tsitouras $\mathbf{5 (4)}$ Runge-Kutta Method~(\Cref{subsec:tsit5_method}) that servers as an excellent default for most ODEs at lower tolerances. Finally, we describe $\mathbf{3^{rd}}$ order Adams-Bashforth Method~(\Cref{subsec:vcab3_method}), a multi-step method that we find to be particularly useful for Neural ODEs.

% \subsection{Euler Method}
% \label{subsec:euler_method}

% The forward Euler method is one of the simplest methods to solve an IVP numerically. Consider the Taylor series expansion of the function $\func{z}{t}$ around $t_n$:
% %
% \begin{equation}
%   \func{z}{t + \dt} = \func{z}{t} + \dt \cdot \left. \frac{\partial z}{\partial t} \right|_{t} + \frac{1}{2} \dt^2 \cdot \left. \frac{\partial^2 z}{\partial t^2} \right|_{t} + \bigO{\dt^3}
% \end{equation}
% %
% If we ignore the quadratic and higher order terms, we get the forward Euler method~\citep{euler1824institutionum}:
% %
% \begin{align}
%            & \func{z}{t + \dt} = \func{z}{t} + \dt \cdot \left. \frac{\partial z}{\partial t} \right|_{t} \\
%   \implies & z_{n + 1} = z_n + \dt \cdot \func{f}{z_n, p, t_n}
% \end{align}
% %
% This method is extremely simple however, has an extremely high local truncation error of $\bigO{\dt^2}$. Hence, this method is rarely used in the context of most IVPs and especially not for Neural ODEs.

\subsection{Tsitouras $\mathbf{5 (4)}$ Runge-Kutta (Tsit5) Method}
\label{subsec:tsit5_method}

Runge-Kutta (RK) Methods~\citep{runge1895numerische, kutta1901beitrag} are widely used to approximate the solutions of ODEs numerically. Given a tableau of coefficients $\left\{A, c, b\right\}$, these methods combine $s$ stages to obtain the estimate at $t + \dt$.
%
\begin{align}
   & k_s = \func{f}{t + c_s \cdot \dt, z(t) + \sum_{i = 1}^{s - 1} a_{si} \cdot k_i}                     \\
   & z(t + dt) = z(t) + \dt \cdot \left( \sum_{i = 1}^s b_i \cdot k_i \right) \label{eq:rk_method_tsit5}
\end{align}
%
\citet{tsitouras2011runge} presented a tableau of coefficients for a 6-stage RK method of order $5 (4)$. We have found Tsit5 to be an excellent default for most ODEs (including Neural ODEs) at lower tolerances.

\subsection{$\mathbf{3^{rd}}$ order Adams-Bashforth (VCAB3) Method}
\label{subsec:vcab3_method}

Contrary to RK Methods, Multi-step methods compute $\func{z}{t}$ by efficiently using the information from previous time steps. A linear multi-step method uses linear interpolation to compute $z_{n + 1}$:
%
\begin{equation}
  z_{n + 1} = z_n + \dt \cdot \sum_{i = 0}^{s} \beta_i \cdot \func{f}{z_{n + 1 - i}, p, t_{n + 1 - i}} \qquad \texttt{given } \sum_{i = 0}^s \beta_i = 1
\end{equation}
%
where $s$ is the number of steps. If $\beta_0 = 0$, then we have an explicit method. In this thesis, we will focus on Adams methods which involve solving the following:
%
\begin{equation}
  z_{n + 1} = z_n + \int_{t_n}^{t_{n + 1}} \func{f}{z(\tau), p, \tau} \cdot \mathrm{d}\tau
\end{equation}
%
Adams methods approximate the integral using polynomial interpolation of the function $f$ using evaluations at points $\left\{ t_{n + 1 - s}, t_{n + 2 - s}, \dots, t_{n} \right\}$. Adams-Bashforth Method approximates the function using Lagrange Interpolation:
%
\begin{equation}
  \func{f}{z(\tau), p, \tau} \approx \sum_{i = 0}^{s} \mathcal{L}_{n + 1 - i} \cdot \func{f}{z_{n + 1 - i}, p, t_{n + 1 - i}}
\end{equation}
%
where $\mathcal{L}_{n + 1 - i}$'s are the Lagrange polynomials. For $s = 3$, we get the $\mathbf{3^{rd}}$ order Adams-Bashforth (VCAB3) Method~\citep{durran1991third}:
%
\begin{equation}
  z_{n + 1} = z_n + \frac{\dt}{12} \cdot \left( 23 \func{f}{z_n, p, t_n} - 16 \func{f}{z_{n - 1}, p, t_{n - 1}} + 5 \func{f}{z_{n - 2}, p, t_{n - 2}} \right)
\end{equation}
%
This method has a local truncation error of $\bigO{\dt^3}$. Additionally, since this method evaluates $f$ infrequently (by reusing the evaluations of $f$ from previous time steps), it is efficient for ODEs with expensive evaluations of $f$ like a Neural ODE.

\section{Adaptive Time-Stepping in Numerical ODE Solvers}
\label{sec:adaptive_time_stepping}

Adaptive solvers need to maximize the step size ($\dt$) while keeping the error estimate below the user-specified tolerances, i.e., they need to satisfy the:
%
\begin{equation}
  \eest \leq \atol + \texttt{max}\left( |z(t)|, |z(t + \dt)|\right) \cdot \rtol
\end{equation}
%
where $\eest$ is the local error estimate. Adaptive solvers utilize an additional linear combiner $\Tilde{b}_i$ to get an alternate solution, typically with one order less convergence~\citep{wanner1996solving, fehlberg1968classical, dormand1980family,tsitouras2011runge}.
%
\begin{equation}
  \Tilde{z}(t + dt) = z(t) + \dt \cdot \left( \sum_{i = 1}^s \Tilde{b}_i \cdot k_i \right)
\end{equation}
%
A classic result from Richardson extrapolation shows that $\eest = \| \Tilde{z}(t + \dt) - z(t + \dt) \|$ is an estimate of the local truncation error~\citep{hairer1, ascher1998computer}. The new step size is determined using the following:
%
\begin{equation}
  q = \left\| \frac{\eest}{\atol + \texttt{max}\left( |z(t)|, |z(t + \dt)|\right) \cdot \rtol} \right\|
\end{equation}
%
\begin{itemize}
  \item If $q < 1$, $\dt$ is accepted.
  \item Otherwise, it is rejected and reduced. A standard PI controller proposes the new step size to be:
        %
        \begin{equation}
          dt_{new} = \eta \cdot q_{n - 1}^\alpha \cdot q_{n}^\beta \cdot dt
        \end{equation}
        %
        where $\eta$ is the safety factor, $q_{n - 1}$ is the error proportion of the previous step, and $(\alpha, \beta)$ are tunable PI-gain hyperparameters~\cite{wanner1996solving}.
\end{itemize}

We defer the discussion of error estimation schemes for stochastic RK integrators to  \citet{rackauckas2017adaptive, rackauckas2020sosri}.

\section{Automatic Stiffness Detection}
\label{sec:automatic_stiffness_detection}

While there is no precise definition of stiffness, the definition used in practice is ``stiff equations are problems for which explicit methods don't work''~\citep{wanner1996solving,shampine1979user}. A simplified stiffness index is given by:
%
\begin{equation}
  S = \text{max}\|\texttt{Re}(\lambda_i)\|
\end{equation}
%
where $\lambda_i$ are the eigenvalues of the local Jacobian matrix. We note that various measures of stiffness have been introduced over the years, all being variations of conditioning of the pseudo-spectra~\citep{shampine2007stiff, higham1993stiffness}. The difficulty in defining a stiffness metric is that in each case, stiff systems like the classic Robertson chemical kinetics or excited Van der Pol equation may violate the definition, meaning all such definitions are (useful) heuristics. In particular, it was shown that for explicit Runge-Kutta methods satisfying $c_x = c_y$ for some internal step, the term
%
\begin{equation}
  \|\lambda\| \approx \left\Vert\frac{ \func{f}{t + c_x \cdot \dt,\sum_{i=1}^{s} a_{xi}} - \func{f}{t + c_y \cdot \dt,\sum_{i=1}^{s} a_{yi}}}{\sum_{i=1}^{s} a_{xi} - \sum_{i=1}^{s} a_{yi}}\right\Vert
\end{equation}
%
serves as an estimate to $S$~\citep{shampine1977stiffness}. Since each of these terms are already required in the Runge-Kutta updates of \Cref{eq:rk_method_tsit5}, this gives a computationally-free estimate. This estimate is thus found throughout widely used explicit Runge-Kutta implementations, such as by the dopri method (found in suites like SciPy and Octave) to exit when stiffness is detected automatically~\citep{wanner1996solving}, and by switching methods which automatically change explicit Runge-Kutta methods to methods more suitable for stiff equations~\citep{rackauckas2019confederated}.

\section{Sensitivity Analysis of ODEs}
\label{sec:sensitivity_analysis_odes}

\begin{table}[t]
  \centering
  \adjustbox{max width=\textwidth}{
    \centering
    \begin{tabular}{lcc}
      \toprule
      \thead{Sensitivity Algorithm}                       & \thead{Memory Requirement}                 & \thead{Memory Requirement with Checkpointing} \\
      \midrule
      Backsolve Adjoint \citep{chen2018neural}            & $\bigO{s}$                                 & $\bigO{s \times c}$                           \\
      Interpolating Adjoint \citep{hindmarsh2005sundials} & $\bigO{s \times t}$                        & $\bigO{s \times c}$                           \\
      Quadrature Adjoint \citep{kim2021stiff}             & $\bigO{\left(s + p\right) \times t}$       & -                                             \\
      Direct Reverse Mode Differentiation                 & $\bigO{s \times t \times \mathrm{stages}}$ & -                                             \\
      \bottomrule
    \end{tabular}
  }
  \caption{\textbf{Memory Requirements for various Sensitivity Algorithms for ODEs}: $s$ is the number of states, $t$ is the number of time steps for the ODE Solve, $c$ is the number of checkpoints ($c << t$) and stages is the number of stages of the ODE solver.}
  \label{tab:memory_requirements_sensitivity_analysis_odes}
\end{table}



\subsection{Continuous Sensitivity Analysis}
\label{subsec:continuous_sensitivity_analysis_odes}

A detailed discussion of different continuous sensitivity algorithms is beyond the scope of this thesis. Instead, we present the algorithm of one of the popular methods (especially for training Neural ODEs) -- Backsolve Adjoint. We refer the readers to \citet[Appendix B]{chen2018neural} for rigorous proof of this method.

Let the gradient of the loss $\mathcal{L}$ wrt the final state of the ODE be $z(t_1)$ be $\frac{\partial L}{\partial z(t_1)}$. Define $\lambda(t)$ to be an augmented state. To compute the adjoint, we solve an augmented ODE from $t_1$ to $t_0$ with the initial condition:
%
\begin{equation}
  s_0 = \left[ z(t_1), \frac{\partial L}{\partial z(t_1)}, 0_{|p|}, -\left(\frac{\partial L}{\partial z(t_1)}\right)^T \func{f}{z(t_1), p, t_1}\right]
\end{equation}
%
The augmented dynamics is given by the following:
%
\begin{align}
  \frac{dz(t)}{dt}         & = \func{f}{z(t), p, t}                                           \\
  \frac{d\lambda(t)}{dt}   & = -\lambda(t)^T \frac{\partial \func{f}{z(t), p, t}}{\partial z} \\
  \frac{d\lambda_p(t)}{dt} & = -\lambda(t)^T \frac{\partial \func{f}{z(t), p, t}}{\partial p} \\
  \frac{d\lambda_t(t)}{dt} & = -\lambda(t)^T \frac{\partial \func{f}{z(t), p, t}}{\partial t}
\end{align}
%
After solving this augmented ODE, we compute the final gradients as:
%
\begin{align}
  \frac{d\mathcal{L}}{dp} = \lambda_p(t_0)   & \qquad \frac{d\mathcal{L}}{dz(t_0)} = \lambda(t_0) \\
  \frac{d\mathcal{L}}{dt_0} = \lambda_t(t_0) & \qquad \frac{d\mathcal{L}}{dt_1} = \lambda_t(t_1)
\end{align}
%
In this algorithm, we don't need to store any of the intermediate activations in the forward solve, hence it is extremely memory efficient and has a complexity of $\bigO{s}$ (See \Cref{tab:memory_requirements_sensitivity_analysis_odes}).

\subsection{Discrete Sensitivity Analysis}
\label{subsec:discrete_sensitivity_analysis_odes}

Discrete Sensitivity Analysis is the same as running any reverse mode AD software directly through the solver. Hence, we need to store the activations at every time step the function was evaluated making the memory cost of the method extremely high. This method doesn't scale too well for larger systems, however, has certain nice properties that can be exploited to accelerate Neural ODEs (See \Cref{sec:discussion_on_global_regularization_of_neural_des}).

\subsection{Tradeoffs between Continuous and Discrete Sensitivity Analysis}
\label{subsec:discussion_on_sensitivity_analysis_optimize_then_discretize_vs_discretize_then_optimize}

Using continuous sensitivity analysis or Optimize-then-Discretize (Opt-Disc) we ``optimize the continuous ODE and then discretize the optimal dynamics after training''~\citep{onken2020discretize}. Alternatively in discrete sensitivity analysis or Discretize-then-Optimize (Disc-Opt) we discretize the ODE and then compute the sensitivities over that discretization. Opt-Disc has a clear memory advantage over Disc-Opt (See \Cref{tab:memory_requirements_sensitivity_analysis_odes}). \citet{gholami2019anode} compared Disc-Opt and Opt-Disc and concluded the following for image classification tasks:
%
\begin{enumerate}
  \item Opt-Disc leads to numerical instabilities in general deep neural network operations like convolutions, etc.
  \item Inconsistent gradients in Opt-Disc can lead to divergence
\end{enumerate}
%
\citet{onken2020discretize} extended this discussion to time-series problems and continuous normalizing flows and showed that Disc-Opt obtains similar or better results compared to Opt-Disc while having a lower computational cost.

\section{Neural Ordinary Differential Equations}
\label{sec:neural_odes}

Neural Ordinary Differential Equations are Implicit Neural Networks that use a neural network to parameterize the dynamics of the ODE: Mathematically, this is given by:
%
\begin{align}
  \frac{dz}{dt} & = \func{f}{z(t), \theta, t} \qquad z(t_0) = z_0                     \\
  \func{z}{t_1} & = \func{z}{t_0} + \int_{t_0}^{t_1} \func{f}{z(t), \theta, t} \, \dt
\end{align}
%
where $f$ is an Explicit Neural Network, $\theta$ are the parameters of the neural network, and we want to solve for the dynamics $\func{z}{t}$ in $t \in [t_0, t_1]$. Typically, $z_0$ is specified as the input from the previous layers. Then we can use sensitivity analysis (\Cref{sec:sensitivity_analysis_odes}) to compute $\left. v^T \frac{\partial \func{z}{t}}{\partial z_0} \right|_{t = t_1}$ and $\left. v^T \frac{\partial \func{z}{t}}{\partial g} \right|_{t = t_1}$ (where $v^T$ is obtained from back-propagation on the succeeding layers) and train the neural network end-to-end using gradient descent.

\section{Common Applications of Neural ODEs}
\label{sec:neural_odes_applications}

Neural ODEs have emerged as a direct alternative to explicit neural networks that can automatically adapt its depth to the problem. However, apart from being a continuous replacement to standard explicit models, Neural ODEs have certain specific modelling advantages that make them a good fit for certain problems. In this section, we discuss some of those applications of Neural ODEs.

\subsection{Density Estimation: Continuous Normalizing Flows and FFJORD}
\label{subsec:density_estimation_neural_odes}

Given a random variable $z \sim \mathcal{P}_z$, we can compute the probability distribution of $x = f(z) \sim \mathcal{P}_x$, where $f: \mathbb{R}^D \mapsto \mathbb{R}^D$ is an invertible function, as:
%
\begin{equation}
  \log{p_x(x)} = \log{p_z(z)} - \log{\det{\left| \frac{\partial f(z)}{\partial z} \right|}}
\end{equation}
%
\citet{chen2018neural} use a neural ODE to transform a sample from a simple distribution $\mathcal{P}_{z_0}$ to the target distribution $\mathcal{P}_{z_1}$. The exact log likelihood of the sample $z(t_1)$ is given by an ODE:
%
\begin{equation}
  \log{p_{z_1}(z(t_1))} = \log{p_{z_0}(z(t_0))} - \bigintss_{t_0}^{t_1} \texttt{Tr}\left| \frac{\partial f(z(t), p, t)}{\partial z(t)} \right| \, \dt
\end{equation}
%
During training the Continuous Normalizing Flow (CNF) we want to obtain the sample $z_0$ that generated the data $x \in \mathbb{R}^D$, and the corresponding log probability $\log{p(x)}$. This is done by solving the following equations (note the backward integration from $t_1$ to $t_0$):
%
\begin{align}
  \begin{bmatrix}
    z(t_0) \\
    \log{p_{z_1}(x)} - \log{p_{z_0}(z(t_0))}
  \end{bmatrix} & = \bigints_{t_1}^{t_0} \begin{bmatrix}
                                           \func{f}{z(t), p, t} \\
                                           -\texttt{Tr} \left| \frac{\partial f(z(t), p, t)}{\partial z(t)} \right|
                                         \end{bmatrix} \, \dt \\
  \begin{bmatrix}
    z(t_1) \\
    \log{p_{z_1}(x)} - \log{p_{z_1}(z(t_1))}
  \end{bmatrix} & = \begin{bmatrix}
                      x \\
                      0
                    \end{bmatrix} \qquad \text{initial conditions}
\end{align}
%
Solving this problem reduces the time complexity of $\bigO{(DH + D^3)L}$ (where $H$ is the size of the largest hidden dimension in $f$ and $L$ is the number of transfomations) in Normalizing Flows to $\bigO{(DH + D^2)\hat{L}}$ (where $\hat{L}$ is the number of function evaluations for the CNF). \citet{grathwohl2018ffjord} further improve the time complexity by using the Hutchinson Trace Estimator~\citep{hutchinson1989stochastic} to estimate $\texttt{Tr}\left| \frac{\partial f(z(t), p, t)}{\partial z(t)} \right|$.
%
\begin{align}
  \log{p_{z_1}(z(t_1))} & = \log{p_{z_0}(z(t_0))} - \bigintss_{t_0}^{t_1} \texttt{Tr}\left| \frac{\partial f(z(t), p, t)}{\partial z(t)} \right| \, \dt                                  \\
                        & = \log{p_{z_0}(z(t_0))} - \bigintss_{t_0}^{t_1} \mathbb{E}_{p_\epsilon} \left[ \epsilon^T \frac{\partial f(z(t), p, t)}{\partial z(t)} \epsilon \right] \, \dt \\
                        & = \log{p_{z_0}(z(t_0))} - \mathbb{E}_{p_\epsilon} \left[ \bigintss_{t_0}^{t_1} \epsilon^T \frac{\partial f(z(t), p, t)}{\partial z(t)} \epsilon \, \dt \right]
\end{align}
%
$\epsilon^T \frac{\partial f(z(t), p, t)}{\partial z(t)}$ is a VJP that can be directly obtained using Reverse Mode AD. This transformation further reduces the time complexity to $\bigO{(DH + D)}\hat{L}$.

\subsection{Time Series Predictions}
\label{subsec:time_series_predictions}

Standard Recurrent Neural Networks (RNNs) ignore the time spacing between subsequent predictions, hence when used in the context of time series predictions, they work well then the data is evenly separated. However, for irregularly spaced time series data, RNNs require discretizing the observation times or imputing the data as a preprocessing.

\citet{chen2018neural} use an latent ODE model with an RNN encoder to capture the dynamics of the time series data. They use a Variational AutoEncoder (VAE) and compute the approximate posterior distribution of the latent variables $z$ given the data sequence $x$:
%
\begin{align}
  \func{q}{z_0 \mid \left\{x_i, t_i \right\}_{i = 0}^N} & = \func{\mathcal{N}}{\mu_{z_0}, \sigma_{z_0}}             \\
  \mu_{z_0}, \sigma_{z_0}                               & = \func{\texttt{RNN}}{\left\{x_i, t_i \right\}_{i = 0}^N}
\end{align}
%
The generative process works by integrating a Neural ODE from $t_0$ to $t_N$:
%
\begin{equation}
  \hat{x}_i = \hat{x}_{i - 1} + \bigintssss_{t_{i - 1}}^{t_i} \func{f}{z(t), p, t} \, \dt \qquad \forall i \in [N] \quad x_0 \sim \func{\mathcal{N}}{\mu_{z_0}, \sigma_{z_0}}
\end{equation}
%
\citet{rubanova2019latent} extended this framework to allow encoding irregular spaced data. They used an ODE-RNN encoder, i.e. a Neural ODE that models the hidden state dynamics of the RNN. Let, $h_{t_i}$ be the hidden state of the RNN at time $t_i$. Since the encoding is performed backwards, i.e. from $t_N$ to $t_0$, the hidden state $h_{t_{i - 1}}$ is given by:
%
\begin{equation}
  h_{t_{i - 1}} = h_{t_i} + \bigintssss_{t_i}^{t_{i - 1}} \func{g}{h(t), p, t} \, \dt
\end{equation}
%
Using an ODE-RNN encoder, allows the model to encode non-uniformly spaced data by simply updating the integration time-span of the ODE.

\section{Accelerating Neural ODEs}
\label{sec:accelerating_neural_odes_prior_works}

Neural ODEs are competitive against explicit models in terms of accuracy and outperform them wrt memory requirements. However, their widespread adoption is bottlenecked by higher training costs and eventual slowdown over training due to emergent complicated dynamics. Hence, to make them competitive against explicit models several approaches have been proposed to accelerate Neural ODEs~\citep{finlay2020train, kelly2020learning, ghosh2020steer, poli2020hypersolvers, kidger2021hey, pal2021opening, xia2021heavy, zhuang2021mali, djeumou2022taylorlagrange, pal2023locally}. In this section, we discuss some of these prior works.

\subsection{Taylor Neural ODE}
\label{subsec:taylor_neural_odes}

In \Cref{sec:adaptive_time_stepping}, we discussed adaptive timestepping for RK schemes. A limiting factor in taking large and accurate time steps is the $K^{th}$ order Taylor Coefficients of the solution trajectory. \citet{kelly2020learning} proposed the following regularization scheme to minimize the $K^{th}$ order Taylor Coefficients:
%
\begin{equation}
  \left(\mathcal{R}_{K}\right)_g = \bigintsss_{t_0}^{t_1} \left\|\frac{d^K z(t)}{dt^K}\right\|_2^2 \dt
\end{equation}
%
Computing the $K^{th}$-order gradients for $\frac{d^K z(t)}{dt^K}$ is computationally prohibitive. However, \citet{kelly2020learning} used Taylor-Mode Automatic Differentiation~\citep{griewank2008evaluating, bettencourt2019taylor} to reduce the exponential Time Complexity to $\bigO{K^2}$ or $\bigO{K \log{K}}$ (based on the operations).

\subsection{STEER}
\label{subsec:steer}

Neural ODEs tend to learn more complex dynamics as the training progresses. Since the complexity of the dynamics and time taken by the solver are intrinsically related, the training and inference time grows over training. \citet{ghosh2020steer} stochastically perturb the ending integration time-point of the ODE to allow the Neural ODE to learn simpler dynamics. During training, they reformulate the problem as:
%
\begin{align}
  \func{z}{t_1}      & = \func{z}{t_0} + \int_{t_0}^{T} \func{f}{z(t), p, t} \, \dt \\
  \texttt{where } T  & \sim \mathcal{U}(t_1 - b, t_1 + b)                           \\
  \phantom{where } b & < t_1 - t_0
\end{align}
%

\subsection{Hypersolvers for Neural ODEs}
\label{subsec:hypersolvers_for_neural_odes}

\citet{poli2020hypersolvers} proposed Hypersolvers to speed up inference of Neural ODEs. They trained a hypersolver to augment a base ODE solver to match the truncation error of a more accurate ODE solver. Let,
%
\begin{equation}
  z_{n + 1} = z_n + \dt \cdot \psi(z_n, p, t_n)
\end{equation}
%
where $\psi$ is the update from an Explicit ODE Integrator. We can represent the steps of a $\gamma^{th}$ order Hypersolved Neural ODE to be:
%
\begin{equation}
  z_{n + 1} = z_n + \dt \cdot \psi(z_n, p, t_n) + \dt^{\gamma + 1} \cdot g_\omega(z_n, z_0, \dt, t_n)
\end{equation}
%
where $g_\omega$ is a neural network that is trained to match the residual of the base ODE solver and a more accurate solver. The typical training process involves:
%
\begin{itemize}
  \item Train a Neural ODE with an accurate ODE solver
  \item Fixed the weights of the Neural ODE
  \item Train the Hypersolver to match the residual of the Neural ODE solved using the base ODE integrator with hypersolver and the more accurate ODE solver.
\end{itemize}
%
\citet{poli2020hypersolvers} demonstrated that Hypersolvers can speedup inference of Neural ODEs by 8x over Dopri5~\citep{dormand1980family} for certain image classification tasks.

\section{Conclusion}
\label{sec:discussion_neural_ode}

In this chapter, we have have briefly covered numerical methods to solve ODEs specifically in the context of Neural ODEs. Additionally, we have described some algorithms for sensitivity analysis of ODEs that allow us to compute gradients and perform gradient based optimization of Neural ODEs. We covered various applications of Neural ODEs like time series modelling and generative modelling. Finally, we have discussed prior works that have accelerated the training and inference of Neural ODEs.

However, most of these prior works have focused on either accelerating the training or the inference. Additionally, speeding up inference often comes at the price of slower training or tools that are not easily applicable. In this thesis, we will describe methods that can accelerate both training and inference of Neural ODEs while being easily composable with existing methods.
