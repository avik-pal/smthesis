\chapter{Introduction}
\label{chapter:introduction}

The combination of numerical methods and scientific computing with machine learning holds immense potential for democratizing the field. Developing neural network architectures that automatically adapt to new problems is crucial in advancing machine learning algorithms. How many hidden layers should you choose in your recurrent neural network? \citet{chen2018neural} showed that the answer could be found automatically by using a continuous reformulation, the neural ordinary differential equation, and allowing an adaptive ODE solver to effectively choose the number of steps to take. Explicit models maximize performance on a dataset by being tuned to the ``hardest'' training sample, which hurts the inference timings for ``easier'' -- more abundant -- samples. Using adaptive differential equation solvers allows these implicit models to choose the number of steps they need effectively. Additionally, these models have constant memory overhead for training, i.e., irrespective of the number of steps of the solver, backpropagation takes constant memory.

Implicit Machine Learning has emerged as a popular modeling framework, enabling ML practitioners to design neural networks that can adaptively modify their depth based on the input problem. The ``depth'' of these models is typically a function of the number of internal explicit neural network evaluations. In this thesis, we will focus on two main types of implicit models -- Deep Equilibrium Models (DEQs)~\citep{bai_deep_2019} drive a discrete dynamical system parameterized by a neural network to steady state, and Neural Ordinary Differential Equations (Neural ODEs)~\citep{chen2018neural} solve an ODE parameterized by a neural network over a fixed time-span. There are several extensions/generalizations to these frameworks -- like Multiscale DEQs~\citep{bai_multiscale_2020}, Neural Stochastic Differential Equations~\citep{liu2019neural}, Universal Differential Equations~\citep{rackauckas2020universal}, etc. -- and we will show that our proposed methods naturally extend to these setups.

However, there is no free lunch. Despite much research in this domain, one fact remained: \textit{solving a neural differential equation is expensive, and training a neural differential equation is even more so}. \citet{grathwohl2018ffjord, dupont2019augmented, kelly2020learning, finlay2020train} have identified several problems with training implicit networks. Implicit Models come with extremely high training and inference costs making them not as scalable as explicit models like transformers~\citep{chen2016attention, vaswani2017attention}, recurrent neural networks~\citep{elman1990finding, hochreiter1997long}, etc. Accelerating forward pass is as simple as demanding the solver to ``take fewer steps.'' But this ``simple objective'' is non-differentiable. Several solutions have been proposed using proxy losses for this objective -- \citet{kelly2020learning, finlay2020train} use higher order derivatives for regularization of Neural Differential Equations, \citet{pal2021opening} proposed a ``zero-cost'' global regularization scheme for problems using adaptive differential equation solvers, and \citet{bai2021stabilizing} use jacobian regularization for Deep Equilibrium Models. Other alternate strategies have been learning new solvers~\citep{poli2020hypersolvers}, randomizing the timespan for integration to smoothen the learned dynamics~\citep{ghosh2020steer}. We build upon prior works to  propose a generally applicable Neural ODE regularization scheme and a infinite time Neural ODE which provide faster training and inference compared to existing methods.

\section{Outline of the Thesis}

In \Cref{part:foundations}, we describe the background needed to understand the novel methods proposed in this thesis. \Cref{chapter:neural_ode} describe the background on Ordinary Differential Equations and Neural Ordinary Differential Equations. \Cref{chapter:deep_equilibrium_models} describes the background on Deep Equilibrium Models or Infinite Depth Neural Networks. We describe our methods in \Cref{part:accelerating_nde}. \Cref{chapter:infinite_time_neural_odes} introduces Infinite-Time Neural ODEs which accelerate the training of Neural ODEs by integrating them to infinity. In \Cref{chapter:internal_solver_heuristics_regularized_neural_des} we open black box differential equation solvers and use internal solver heuristics and discrete sensitivity analysis to accelerate the training and inference of Neural Differential Equations. In \Cref{chapter:local_regularization_neural_odes} we observe the shortcomings of former global regularization schemes and propose a local regularization scheme for Neural ODEs. In \Cref{part:open_source_software}, we describe the open-source softwares developed as a part of this thesis work. Finally we conclude the thesis with open future research directions in \Cref{part:conclusion_and_future_work}.

\section{Main Contributions}

The main contributions of this thesis are as follows:
%
\begin{enumerate}
  \item In \Cref{chapter:infinite_time_neural_odes}, we will design a \textit{continuous variant of Deep Equilibrium Networks} that is equivalent to integrating a Neural ODE to \textit{infinite}.
  \item In \Cref{chapter:infinite_time_neural_odes}, we will demonstrate empirically and mathematically that \textit{integrating a Neural ODE to infinity} accelerates the backwards pass.
  \item In \Cref{chapter:internal_solver_heuristics_regularized_neural_des}, we will build upon the internals of differential equation solvers and open the solver black-box to \textit{accelerate training and inference of Neural Differential Equations}.
  \item In \Cref{chapter:local_regularization_neural_odes}, we will further show that trading off slight performance allows us to develop a \textit{highly-composable stochastic local regularization scheme} for Neural Differential Equations.
  \item In \Cref{chapter:lux_bridging_scientific_computing_and_deep_learning}, we introduce a new Julia deep learning library, \textit{Lux}, that bridges the gap between scientific computing and deep learning.
\end{enumerate}
